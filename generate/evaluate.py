"""
Question Quality Evaluation Module
=================================

This module handles the evaluation of clarifying questions generated by models.
It provides functionality to assess question quality and generate answers to
clarifying questions using different evaluation protocols.

Key functions:
- evaluate_clarifying_questions: Main evaluation function with multiple protocols
- call_gemini: Interface to Google Gemini API for evaluation
- call_chatgpt_o1: Interface to OpenAI GPT-4o-mini for evaluation

Evaluation protocols supported:
- Standard evaluation with regex parsing
- LLM-based evaluation (llm_metric_v2)
"""

from config import PROMPT_LLM_BASED_COMM_RATE, PROMPT_EVALUATE_QUESTION_QUALITY, client
from utils import call_chatgpt_o1
from prompt import load_prompt_from_config
import re
from process import print_file
from main import model
import openai

def evaluate_clarifying_questions(
    missing_information='',
    clarifying_questions='',
    problem='',
    eval_protocol='',
):
    """
    Evaluate the quality of clarifying questions and generate answers.
    
    Args:
        missing_information (str): The original, complete problem description
        clarifying_questions (str): The clarifying questions generated by the model
        problem (str): The modified/incomplete problem description
        eval_protocol (str): Evaluation protocol to use ('llm_metric_v2' for LLM-based evaluation)
    
    Returns:
        tuple: (answer_string, quality_score) where:
            - answer_string: Generated answer to the clarifying questions
            - quality_score: Quality rating of the questions (0-3 scale)
    """
    print("\n\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", file=print_file)
    print('!!!!!!! 2nd evaluate_clarifying_questions START !!!!!!!!!!!', file=print_file)
    
    # ============================================================================
    # LLM-BASED EVALUATION PROTOCOL (llm_metric_v2)
    # ============================================================================
    if eval_protocol == 'llm_metric_v2':
        # Step 1: Determine if the response is code (0) or questions (1)
        prompt_comm_rate = PROMPT_LLM_BASED_COMM_RATE.format(
            clarifying_questions=clarifying_questions
            )
        comm_rate = call_chatgpt_o1(prompt_comm_rate)
        
        # Step 2: If questions were generated, evaluate their quality
        if str(comm_rate) == "1":
            prompt_qq = PROMPT_EVALUATE_QUESTION_QUALITY.format(
                    missing_information=missing_information,
                    clarifying_questions=clarifying_questions,
                    problem=problem
                )
            quality = call_chatgpt_o1(prompt_qq)
        else:
            # If no questions were generated, quality is 0
            quality = 0
            
        # Format the result string
        answer_str = "comm_rate_" + str(comm_rate) + "_question_quality_v2_" + str(quality)
        
        print('!!!!!!!answer_str',answer_str, file=print_file)
        print('!!!!!!!question_quality_str',quality, file=print_file)
        
        print('!!!!!!! 2nd evaluate_clarifying_questions END !!!!!!!!!!!', file=print_file)
        print("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n\n", file=print_file)
        return answer_str, quality
    
    # ============================================================================
    # STANDARD EVALUATION PROTOCOL (Default)
    # ============================================================================
    # Use GPT-3.5-turbo for standard evaluation with regex parsing
    
    topn = 1
    temperature = 1.0
    model = 'gpt-3.5-turbo-0125'  # Default model for evaluation
    prompt_evaluate_questions = load_prompt_from_config(phase = 2)
    
    # Format the evaluation prompt with the provided information
    content = prompt_evaluate_questions.format(
                missing_information=missing_information,
                clarifying_questions=clarifying_questions,
                problem=problem
            )
    
    # Call OpenAI API for evaluation
    completion = openai.ChatCompletion.create(
        model=model,
        n=topn,
        temperature=temperature,
        messages=[{
            "role": "user",
            "content": content,
        }]
    )
    
    print('!!!!!!!PROMPT_EVALUATE_QUESTIONS='+content, file=print_file)
    print('!!!!!!!Completion='+completion['choices'][0]['message']['content'], file=print_file)
    
    # ============================================================================
    # RESPONSE PARSING
    # ============================================================================
    # Extract quality score and answers using regex patterns
    completion_content = str(completion['choices'][0]['message']['content'])

    # Extract quality score (integer following "QUALITY=")
    question_quality = re.findall(r'QUALITY\s*=?\s*(\d+)', completion_content)
    
    # Extract answers (text within triple backticks following "ANSWERS=")
    answers = re.findall(r'ANSWERS\s*=?\s*```(.+?)```', completion_content, flags=re.DOTALL)
    
    # Extract the first answer and quality score, or use empty string if not found
    answer_str = answers[0] if answers else ""
    question_quality_str = question_quality[0] if question_quality else ""
    
    print('!!!!!!!answer_str',answer_str, file=print_file)
    print('!!!!!!!question_quality_str',question_quality_str, file=print_file)
    
    print('!!!!!!! 2nd evaluate_clarifying_questions END !!!!!!!!!!!', file=print_file)
    print("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n\n", file=print_file)
    return answer_str, question_quality_str





# ============================================================================
# HELPER FUNCTIONS FOR API CALLS
# ============================================================================

def call_gemini(prompt):
    """
    Call Google Gemini API for evaluation tasks.
    
    Args:
        prompt (str): The prompt to send to Gemini
    
    Returns:
        int: Parsed integer response from Gemini, or -1 if parsing fails
    """
    response = model.generate_content(prompt)
    try:
        return int(response.text.strip())
    except ValueError:
        return -1

def call_chatgpt_o1(prompt):
    """
    Call OpenAI GPT-4o-mini API for evaluation tasks.
    
    Args:
        prompt (str): The prompt to send to GPT-4o-mini
    
    Returns:
        int: Parsed integer response from GPT-4o-mini, or -1 if parsing fails
    """
    completion = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {
                "role": "user",
                "content": prompt
            }
        ]
    )
    try:
        return int(completion.choices[0].message.content.strip())
    except ValueError:
        return -1