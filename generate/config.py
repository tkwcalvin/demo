"""
Configuration and Constants Module
==================================

This module contains all configuration settings, API keys, and prompt templates
used throughout the HumanEval code generation system.

Key components:
- API configurations (OpenAI, Google Gemini)
- Prompt templates for different model types and tasks
- Model-specific formatting constants
- Evaluation prompts for question quality assessment

Dependencies:
- openai: For GPT model API calls
- google.generativeai: For Gemini model API calls
- transformers: For random seed setting
"""

# -*- coding: utf-8 -*-
import openai
import os
from transformers import set_seed
import google.generativeai as genai
from openai import OpenAI

# ============================================================================
# GLOBAL CONFIGURATION
# ============================================================================
# Set random seed for reproducibility across all experiments
set_seed(42)

# ============================================================================
# AGENT FRAMEWORK IMPORTS (OPTIONAL)
# ============================================================================
# Note: CodeGeeX imports are commented out due to module availability issues.
# Re-enable these imports and install CodeGeeX if running AgentCoder experiments.
# See: https://github.com/jie-jw-wu/human-eval-comm/blob/main/README_AgentFramework.md
#from AgentFramework.programmer import programmer_main
#from AgentFramework.designer import designer_main
#from AgentFramework.executor import executor_main

# ============================================================================
# MODEL-SPECIFIC FORMATTING CONSTANTS
# ============================================================================
# CodeLlama instruction formatting tokens
B_INST_CLLAMA, E_INST_CLLAMA = "[INST]", "[/INST]"
B_SYS_CLLAMA, E_SYS_CLLAMA = "<<SYS>>\n", "\n<</SYS>>\n\n"

# ============================================================================
# API CONFIGURATION
# ============================================================================
# OpenAI API configuration
# openai.api_key = os.environ['OPENAI_KEY']
openai.api_key = os.environ['OPENAI_API_KEY']  # Alternative environment variable name
client = OpenAI()

# Google Gemini API configuration
gemini_api_key = os.getenv("GEMINI_API_KEY")
genai.configure(api_key=gemini_api_key)
gemini_model = genai.GenerativeModel("gemini-pro")
# ============================================================================
# PROMPT TEMPLATES FOR CODE GENERATION
# ============================================================================
# These templates define how models are prompted to generate code or ask questions

# Basic code generation prompts
PROMPT_START_0 = 'Generate Python3 code (Markdown):\n'
PROMPT_START_1 = 'Generate either Python3 code only (Markdown) or no code:\n'
PROMPT_START_2 = 'Generate either Python3 code only (Markdown) or ask questions:\n'

# Advanced prompts that allow both code generation and clarifying questions
PROMPT_START_3 = 'You are an expert software developer. Generate Python3 code (code must has Markdown in response) in below information. Alternatively, you can ask clarifying questions: \n'
PROMPT_START_3_v2 = 'You are an expert software developer who writes high quality code. With below information, please either generate Python3 code (Respond directly with code only with markdown), or ask clarifying questions: \n'

# Experimental prompts (marked for future testing)
PROMPT_START_3_v3 = 'You are an expert software developer who writes high quality code. With below information, please either generate Python3 code (only one code block with markdown in response), or ask clarifying questions (no markdown in response): \n'
PROMPT_START_3_v4 = '\n Based on the information below, you can choose to either generate Python3 code (Respond directly with code only with markdown), or ask clarifying questions. \n'

# Original prompt for standard code generation (no questions allowed)
ORIGINAL_PROMPT_START_0 = 'You are an expert software developer who writes high quality code. With below information, please generate Python3 code (Respond directly with code only with markdown): \n'

# ============================================================================
# EVALUATION PROMPTS FOR QUESTION QUALITY ASSESSMENT
# ============================================================================
# These prompts are used to evaluate the quality of clarifying questions generated by models

# Version 1: Standard evaluation prompt with 3-point scale
PROMPT_EVALUATE_QUESTIONS_V1 = 'The original description of a coding problem is modified so that the requirements become inconsistent, incomplete, or ambiguous. Given the modified description, some clarifying questions were raised to clarify the description. Given the original and modified problem description, evaluate the quality of the questions. Please provide an integer representing the quality of questions (3: Good questions that recover all missing info. 2: Fair questions that recover some missing info. 1: Bad questions or irrelevant content).\n  QUALITY=[your int] \n Please also provide answers to the questions to recover the missing requirements! Be sure to add what is new or different in the original descrpition in your answer, compared with the modified problem description! \n ANSWERS=```[your answer]```  \n Please strictly follow the format QUALITY=[the int] and ANSWERS=```[the answer]``` in the response! Surround your answer with markup! \n\n ### Questions: {clarifying_questions} \n ### Problem Description: {problem} \n ### Original Description: {missing_information} \n'

# Version 2: Similar to V1 but with slight modifications
PROMPT_EVALUATE_QUESTIONS_V2 = 'The original description of a coding problem is modified so that the requirements become inconsistent, incomplete, or ambiguous. Given the modified description, some clarifying questions were raised to clarify the description. Given the original and modified problem description, evaluate the quality of the questions. Please provide an integer representing the quality of questions (3: Good questions that recover all missing info. 2: Fair questions that recover some missing info. 1: Bad questions or irrelevant content).\n  QUALITY=[your int] \n Please also provide answers to the questions to recover the missing requirements! Be sure to add what is new or different in the original descrpition in your answer, compared with the modified problem description! \n ANSWERS=```[your answer]```  \n Please strictly follow the format QUALITY=[the int] and ANSWERS=```[the answer]``` in the response! Surround your answer with markup! \n\n ### Questions: {clarifying_questions} \n ### Problem Description: {problem} \n ### Original Description: {missing_information} \n'

# Version 3: Problematic prompt that returns code in answers (marked as "pretty bad")
PROMPT_EVALUATE_QUESTIONS_V3 = 'The original description of a coding problem is modified so that the requirements become incomplete, inconsistent, or ambiguous. Given the modified description, some clarifying questions may be raised to clarify the description. Provide answers to the questions to recover the requirements in the original problem description compared to the modified one. Be sure to return empty answers if there is no valid clarifying question or code with markup! \n ANSWERS=```[your answer]```  \n Please also provide an integer representing the quality of clarifying questions (3: Good questions that recover the modified requirements. 2: Fair questions but they cannot help recover the modified requirements. 1: No valid questions).\n  QUALITY=[your int] \n Please strictly follow the format ANSWERS=```[the answer]``` and QUALITY=[the int] in the response! Surround your answer with markup! \n ### ORIGINAL PROBLEM DESCRIPTION:\n {missing_information} \n ### MODIFIED PROBLEM DESCRIPTION:\n {problem} \n ### CLARIFYING QUESTIONS:\n{clarifying_questions} \n'

# Version 4: Experimental prompt (disabled due to poor performance)
# Note: This prompt was tried but produced no Quality=0 scores for 5 problems, so it's disabled
PROMPT_EVALUATE_QUESTIONS_V4 = 'The original description of a coding problem is modified so that the requirements become inconsistent, incomplete, or ambiguous. Given the modified description, some clarifying questions were raised to clarify the description. Given the original and modified problem description, evaluate the quality of the clarifying questions. Please provide an integer representing the quality of questions (3: Good questions that recover the modified requirements; 2: Fair questions but they cannot help recover the modified requirements; 1: No questions).\n  QUALITY=[your int] \n Please also provide answers to the clarifying questions to recover the modified requirements in the original problem description compared to the modified one. If there is no clarifying questions at all, return empty answers. \n ANSWERS=```[your answer]```  \n Please strictly follow the format QUALITY=[the int] and ANSWERS=```[the answer]``` in the response! Surround your answer with markdown! \n\n ### Questions: Certainly! Can you please provide more specific details on what the `candidate` function needs to do with the input list of strings and the string `x`? This will help me generate the Python3 code according to your requirements. \n ### Modified Problem Description: def candidate(strings: List[str], x: str) -> List[str]:\n    """ Process an input list of strings\n    """ \n ### Original Description: def filter_by_substring(strings: List[str], substring: str) -> List[str]:\n    """ Filter an input list of strings only for ones that contain given substring\n    >>> filter_by_substring([], \'a\')\n    []\n    >>> filter_by_substring([\'abc\', \'bacd\', \'cde\', \'array\'], \'a\')\n    [\'abc\', \'bacd\', \'array\']\n    """ \n ### Response: QUALITY=3\nANSWERS=```The `candidate` function needs to filter the input list of strings such that only strings containing the substring `x` are included in the output list.```\n\n ### Questions: Sure, I can help you with that! Here\'s the Python 3 code for the `candidate` function:\n```python\ndef candidate(num):\n    if num < 0:\n        return (1, 1)\n    else:\n        return (1, num)\n```\n ### Modified Problem Description: def candidate(num):\n    """Example:\n        candidate(-12) ==> (1, 1)\n        candidate(123) ==> (1, 2)\n    """\n ### Original Description: def even_odd_count(num):\n    """Given an integer. return a tuple that has the number of even and odd digits respectively.\n     Example:\n        even_odd_count(-12) ==> (1, 1)\n        even_odd_count(123) ==> (1, 2)\n    """\n ### Response: QUALITY=1\nANSWERS=```no questions```\n\n ### Questions: {clarifying_questions} \n ### Modified Problem Description: {problem} \n ### Original Description: {missing_information} \n ### Response: \n'

# Current standard evaluation prompt (used by default)
PROMPT_EVALUATE_QUESTIONS = 'The original description of a coding problem is modified so that the requirements become inconsistent, incomplete, or ambiguous. Given the modified description, some clarifying questions were raised to clarify the description. Given the original and modified problem description, evaluate the quality of the clarifying questions. Please provide an integer representing the quality of questions (3: Good questions that recover the modified requirements; 2: Fair questions but they cannot help recover the modified requirements; 1: No questions).\n  QUALITY=[your int] \n Please also provide answers to the clarifying questions to recover the modified requirements in the original problem description compared to the modified one. If there is no clarifying questions at all, return empty answers. \n ANSWERS=```[your answer]```  \n Please strictly follow the format QUALITY=[the int] and ANSWERS=```[the answer]``` in the response! Surround your answer with markdown! \n\n ### Questions: {clarifying_questions} \n ### Modified Problem Description: {problem} \n ### Original Description: {missing_information} \n'

# ============================================================================
# LLM-BASED EVALUATION PROMPTS
# ============================================================================
# These prompts are used for automated evaluation using LLMs

# Binary classification: determine if response is code (0) or question (1)
PROMPT_LLM_BASED_COMM_RATE = 'Is the following response a code or a question? Respond with 0 for code and 1 for question.\n\nResponse:\n{clarifying_questions}'

# Quality evaluation: assess question quality on 1-2 scale
PROMPT_EVALUATE_QUESTION_QUALITY = 'The original description of a coding problem is modified such that the modified version is either  inconsistent, incomplete, ambiguous, or a combination of any of them. The modified problem is provided to an expert, where he will either answer with a question asking for clarifications or a direct code answer. You are provided the original and modified problem descriptions together with the expert answer. Your task is to evaluate the quality of expert questions on a scale from 1 to 2, where 1 indicates that fair questions were asked but they cannot help recover the modified requirements, and 2 indicates good questions that recover the modified requirements. Please respond with a signle integer 1, or 2 based on the grading schema that I outlined. No explanation needed, just a single integer. \n\n ### Questions: {clarifying_questions} \n ### Modified Problem Description: {problem} \n ### Original Description: {missing_information}'

# ============================================================================
# MULTI-ROUND CONVERSATION PROMPTS
# ============================================================================
# Prompts used for second round of code generation after Q&A

# Second round prompt: generate code after receiving clarifying answers
PROMPT_2ND_ROUND = '\n Given above conversations, generate Python code directly (Markdown) to solve the coding problem:\n'

# ============================================================================
# OKANAGAN MODEL PROMPTS
# ============================================================================
# Special prompts for the "Okanagan" model variant

# Code generation prompt for Okanagan
OK_PROMPT_CODEGEN = 'Generate Python code directly (Markdown) to solve the coding problem. \n\n'

# Clarifying questions prompt for Okanagan
OK_PROMPT_CLARIFY_Q = 'Given the programming problem, ask clarifying questions if the requirements in the given problem description are incomplete, inconsistent or ambiguous for solving the problem correctly and passing the tests. \n If no need to ask clarifying questions, return strictly \'NO_QUESTIONS\' only. Otherwise, return the clarifying questions. \n\n ### Problem: \n {problem}'

# Alternative clarifying questions prompt (version 1)
OK_PROMPT_CLARIFY_Q_V1 = 'Given the coding problem description and the generated code above, decide whether to ask clarifying questions that are necessary to solve the problem correctly. \n If no need to ask clarifying questions, return strictly \'NO_QUESTIONS\' only. Otherwise, return the clarifying questions. \n\n'

# Default model for Okanagan experiments
OK_MODEL = 'gpt-3.5-turbo-0125'

# ============================================================================
# MODEL CLASSIFICATION LISTS
# ============================================================================
# Models are categorized by their training approach for proper prompt handling

# Instruction-tuned models: trained with instruction-following capabilities
INSTRUCTION_MODELS = [
    "codellama/CodeLlama-7b-Instruct-hf",
    "codellama/CodeLlama-13b-Instruct-hf", 
    "codellama/CodeLlama-34b-Instruct-hf",
    "HuggingFaceH4/starchat-beta",
]

# Foundation models: base models without instruction tuning
FOUNDATION_MODELS = [
    "bigcode/starcoder",
    "bigcode/starcoderplus",
    "bigcode/starcoderbase",
    "bigcode/starcoderbase-7b",
    "bigcode/starcoderbase-3b",
    "bigcode/starcoderbase-1b",
    "codellama/CodeLlama-7b-hf",
    "codellama/CodeLlama-13b-hf",
    "codellama/CodeLlama-34b-hf",
]


# ============================================================================
# DETAILED PROMPT TEMPLATES FOR SPECIFIC MODELS
# ============================================================================
# These are comprehensive prompt templates used for specific model types
# Each template includes system instructions, examples, and formatting guidelines

# CodeLlama instruction template for Natural Language to Programming Language (NL2PL) tasks
CODELLAMA_NL_2_PL_HUMANEVAL = [
    {  # Instructions
        "role": "system",
        "content": PROMPT_START_3_v2
        + " Note that if you decide to generate code, please respond directly with code only with markdown! You need to return the complete function! Please only return code surrounded by markdown! Don't write down any thought processes!  \n\n",
    },
    {  # One-Shot Example: user input = function signature + problem description in docstring format
        "role": "user",
        "content": 'from typing import List\n\n\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\n    '
        + '"""Check if in given list of numbers, are any two numbers closer to each other than\n    given threshold.\n    '
        + '>>> has_close_elements([1.0, 2.0, 3.0], 0.5)\n    False\n    '
        + '>>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\n    True\n    """\n',
    },
    {  # One-Shot Example: model output = solution
        "role": "assistant",
        "content": '```python\ndef candidate(numbers: List[float], threshold: float) -> bool:\n    for i in range(len(numbers)):\n        for j in range(i+1, len(numbers)):\n            if abs(numbers[i] - numbers[j]) <= threshold:\n                return True\n    return False\n```',
    },
    {  # One-Shot Example: user input = function signature + problem description in docstring format
        "role": "user",
        "content": 'from typing import List\n\n\ndef candidate(...) -> bool:\n \"\"\" Check given a list of number.\"\"\"\n',
    },
    {  # One-Shot Example: model output = solution
        "role": "assistant",
        "content": 'Could you please provide more information on which criteria to check in this function?',
    },
]

CODELLAMA_NL_2_PL_HUMANEVAL_V2 = [
    {  # Instructions
        "role": "system",
        "content": "You are an expert software developer who writes high quality code. Given a coding problem, please either generate Python code, or ask clarifying questions. "
        + "If you decide to generate code: please strictly follow these: Respond directly with code only with markdown! You need to return the complete function! Please only return code surrounded by markdown. Don't write down any thought processes!  \n\n",
    },
]

NL_2_PL_HUMANEVAL = [
    {  # Instructions
        "role": "system",
        "content": "Solve a coding problem in Python. "
        + "Given the function signature and the problem description in the docstring, "
        + "you only need to continue to complete the function body. "
        + "Please strictly follow the format of the example below! "
        + "Don't write down any thought processes! "
        + "Don't copy the problem description! "
        + "You must use correct indentation! "
        + "Make sure your return statement is always inside the function! "
        + "Make sure your output always starts with an indentation of exactly 4 spaces! "
        + "Output an indentation of 4 spaces first before you write anything else! You’d better be sure. \n\n",
    },
    {  # One-Shot Example: user input = function signature + problem description in docstring format
        "role": "user",
        "content": 'from typing import List\n\n\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\n    '
        + '"""Check if in given list of numbers, are any two numbers closer to each other than\n    given threshold.\n    '
        + '>>> has_close_elements([1.0, 2.0, 3.0], 0.5)\n    False\n    '
        + '>>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\n    True\n    """\n',
    },
    {  # One-Shot Example: model output = solution
        "role": "assistant",
        "content": '    sorted_numbers = sorted(numbers)\n    for i in range(len(sorted_numbers) - 1):\n        '
        + 'if sorted_numbers[i + 1] - sorted_numbers[i] < threshold:\n            return True\n    return False\n\n',
    },
    {  # Instructions to emphasize the format
        "role": "system",
        "content": "\nPlease strictly follow the format of the example above! "
        + "You must use correct indentation! "
        + "Make sure your return statement is always inside the function! "
        + "Make sure your output always starts with an indentation of exactly 4 spaces! "
        + "Output an indentation of 4 spaces first before you write anything else! "
        + "You’d better be sure. \n\n",
    },
]

PL_2_NL_HUMANEVAL = [
    {  # Instructions
        "role": "system",
        "content": "Given a Python solution to a coding problem, "
        + "write an accurate problem description for it in the format of Python docstring without 'Args' and 'Returns'. "
        + "Please strictly follow the format of the example below!"
        + "Provide all necessary details to accurately describe the problem, but in a concise way! "
        + "Make sure to give a few examples of inputs and outputs in the docstring! "
        + "Make sure the docstring has no 'Args' and no 'Returns'! "
        + "You can only write a text desciption with a few examples as shown in the example below!  "
        + "Make sure your output always starts with an indentation of exactly 4 spaces! "
        + "You’d better be sure. \n\n",
    },
    {  # One-Shot Example: user input = function signature + candidate solution
        "role": "user",
        "content": 'from typing import List\n\n\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\n    '
        + 'sorted_numbers = sorted(numbers)\n    for i in range(len(sorted_numbers) - 1):\n        '
        + 'if sorted_numbers[i + 1] - sorted_numbers[i] < threshold:\n            return True\n    return False\n\n',
    },
    {  # One-Shot Example: model output = problem description in docstring format
        "role": "assistant",
        "content": '    """Check if in given list of numbers, are any two numbers closer to each other than\n    '
        + 'given threshold.\n    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\n    False\n    '
        + '>>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\n    True\n    """\n',
    },
    {  # Instructions to emphasize the format
        "role": "system",
        "content": "\nPlease strictly follow the format of the example above! "
        + "Provide all necessary details to accurately describe the problem, but in a concise way! "
        + "Make sure to give a few examples of inputs and outputs in the docstring! "
        + "Make sure the docstring has no 'Args' and no 'Returns'! "
        + "You can only write a text desciption with a few examples as shown in the example above!  "
        + "Make sure your output always starts with an indentation of exactly 4 spaces! "
        + "You’d better be sure. \n\n",
    },
]

NL_2_PL_MBPP = [
    {  # Instructions
        "role": "system",
        "content": "Solve a coding problem in Python. "
        + "Given the function signature and the problem description in the docstring, you only need to continue to complete the function body. "
        + "Please strictly follow the format of the example below! "
        + "Don't write down any thought processes! "
        + "Don't copy the problem description! "
        + "You must use correct indentation! "
        + "Make sure your return statement is always inside the function! "
        + "Make sure your output always starts with an indentation of exactly 4 spaces! "
        + "Output an indentation of 4 spaces first before you write anything else! "
        + "You’d better be sure. \n\n",
    },
    {  # One-Shot Example: user input = function signature + problem description in docstring format
        "role": "user",
        "content": 'def similar_elements(test_tup1, test_tup2):\n    '
        + '""" Write a function to find the shared elements from the given two lists.\n    """\n',
    },
    {  # One-Shot Example: model output = solution
        "role": "assistant",
        "content": '    res = tuple(set(test_tup1) & set(test_tup2))\n    return (res)\n\n',
    },
    {  # Instructions to emphasize the format
        "role": "system",
        "content": "\nPlease strictly follow the format of the example above! "
        + "You must use correct indentation! "
        + "Make sure your return statement is always inside the function! "
        + "Make sure your output always starts with an indentation of exactly 4 spaces! "
        + "Output an indentation of 4 spaces first before you write anything else! "
        + "You’d better be sure. \n\n",
    },
]

PL_2_NL_MBPP = [
    {  # Instructions
        "role": "system",
        "content": "Given a Python solution to a coding problem, write an accurate problem description for it in the format of Python docstring"
        + "Please strictly follow the format of the example below!"
        + "Provide all necessary details to accurately describe the problem, but in a concise way! "
        + "Make sure the docstring has no 'Args', no 'Returns', and no 'Examples'! "
        + "You can only write a plain text desciption as shown in the example below! "
        + "Make sure your output always starts with an indentation of exactly 4 spaces! "
        + "You’d better be sure. \n\n",
    },
    {  # One-Shot Example: user input = function signature + candidate solution
        "role": "user",
        "content": 'def similar_elements(test_tup1, test_tup2):\n    res = tuple(set(test_tup1) & set(test_tup2))\n    return (res)\n\n',
    },
    {  # One-Shot Example: model output = problem description in docstring format
        "role": "assistant",
        "content": '    """ Write a function to find the shared elements from the given two lists.\n    """\n',
    },
    {  # Instructions to emphasize the format
        "role": "system",
        "content": "\nPlease strictly follow the format of the example above! "
        + "Provide all necessary details to accurately describe the problem, but in a concise way! "
        + "Make sure the docstring has no 'Args', no 'Returns', and no 'Examples'! "
        + "You can only write a plain text desciption as shown in the example above! "
        + "Make sure your output always starts with an indentation of exactly 4 spaces! "
        + "You’d better be sure. \n\n",
    },
]

ONE_SHOT_HUMANEVAL = (
    'def has_close_elements(numbers: List[float], threshold: float) -> bool:\n    '
    + '"""Check if in given list of numbers, are any two numbers closer to each other than\n    given threshold.\n    '
    + '>>> has_close_elements([1.0, 2.0, 3.0], 0.5)\n    False\n    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\n    '
    + 'True\n    """\n    sorted_numbers = sorted(numbers)\n    for i in range(len(sorted_numbers) - 1):\n        '
    + 'if sorted_numbers[i + 1] - sorted_numbers[i] < threshold:\n            return True\n    return False\n\n'
)

ONE_SHOT_MBPP = (
    'def similar_elements(test_tup1, test_tup2):\n    """ Write a function to find the shared elements from the given two lists.\n    '
    + '"""\n    res = tuple(set(test_tup1) & set(test_tup2))\n    return (res)\n\n'
)




# ============================================================================
# COMMON WORDS FOR TEXT MANIPULATION
# ============================================================================
# List of common words used for text replacement experiments
# These words are used when randomly replacing parts of problem descriptions
# to test model robustness to modified or corrupted input
common_words = [
    "apple", "banana", "chocolate", "dog", "elephant", "flower", "guitar",
    "happiness", "internet", "jazz", "kangaroo", "laughter", "mountain", "ocean",
    "penguin", "question", "rainbow", "sunshine", "umbrella", "victory", "wonderful",
    "xylophone", "zebra", "car", "house", "book", "computer", "chair", "table",
    "phone", "shoes", "music", "friend", "beach", "movie", "cake", "coffee",
    "game", "travel", "nature", "art", "garden", "party", "smile", "love", "star",
    "moon", "bird", "child", "family", "money", "dream", "time", "water", "fire",
    "food", "work", "school", "world", "health", "peace", "joy", "knowledge", "color",
    "forest", "planet", "song", "heart", "adventure", "freedom", "success", "history"
]


HumanEvalComm_prompts = ['prompt1a','prompt1c','prompt1p','prompt2ac','prompt2ap','prompt2cp','prompt3acp']
HumanEval_prompt = ["prompt"]


